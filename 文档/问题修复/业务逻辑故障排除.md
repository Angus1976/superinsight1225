# ä¸šåŠ¡é€»è¾‘åŠŸèƒ½æ•…éšœæ’æŸ¥æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å¿«é€Ÿè¯Šæ–­å’Œè§£å†³ SuperInsight ä¸šåŠ¡é€»è¾‘æç‚¼ä¸æ™ºèƒ½åŒ–åŠŸèƒ½ä¸­å¯èƒ½é‡åˆ°çš„é—®é¢˜ã€‚æŒ‰ç…§é—®é¢˜ç±»å‹åˆ†ç±»ï¼Œæä¾›è¯¦ç»†çš„æ’æŸ¥æ­¥éª¤å’Œè§£å†³æ–¹æ¡ˆã€‚

## å¿«é€Ÿè¯Šæ–­å·¥å…·

### ç³»ç»Ÿå¥åº·æ£€æŸ¥

é¦–å…ˆè¿è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥æ¥å¿«é€Ÿè¯†åˆ«é—®é¢˜ï¼š

```bash
# æ£€æŸ¥ä¸šåŠ¡é€»è¾‘æœåŠ¡çŠ¶æ€
curl -X GET "http://localhost:8000/api/business-logic/health"

# é¢„æœŸå“åº”
{
  "status": "healthy",
  "service": "business-logic",
  "timestamp": "2026-01-05T10:30:00Z",
  "version": "1.0.0"
}
```

### æœåŠ¡ä¾èµ–æ£€æŸ¥

```bash
# æ£€æŸ¥æ•°æ®åº“è¿æ¥
python -c "
from src.database import get_db_connection
try:
    conn = get_db_connection()
    print('âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸')
except Exception as e:
    print(f'âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}')
"

# æ£€æŸ¥ Redis ç¼“å­˜
python -c "
import redis
try:
    r = redis.Redis(host='localhost', port=6379, db=0)
    r.ping()
    print('âœ… Redis è¿æ¥æ­£å¸¸')
except Exception as e:
    print(f'âŒ Redis è¿æ¥å¤±è´¥: {e}')
"

# æ£€æŸ¥ NLP æ¨¡å‹
python -c "
import spacy
try:
    nlp = spacy.load('en_core_web_sm')
    print('âœ… spaCy æ¨¡å‹åŠ è½½æ­£å¸¸')
except Exception as e:
    print(f'âŒ spaCy æ¨¡å‹åŠ è½½å¤±è´¥: {e}')
"
```

## å¸¸è§é—®é¢˜åˆ†ç±»

### 1. åˆ†ææ€§èƒ½é—®é¢˜

#### é—®é¢˜ç—‡çŠ¶
- åˆ†æä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿‡é•¿ (>5åˆ†é’Ÿ)
- ç³»ç»Ÿå“åº”ç¼“æ…¢æˆ–è¶…æ—¶
- å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜

#### æ’æŸ¥æ­¥éª¤

**æ­¥éª¤ 1: æ£€æŸ¥æ•°æ®é‡**
```python
# æ£€æŸ¥é¡¹ç›®æ•°æ®é‡
import pandas as pd
from src.database import get_annotations

def check_data_size(project_id):
    annotations = get_annotations(project_id)
    data_size = len(annotations)
    
    print(f"é¡¹ç›® {project_id} æ•°æ®é‡: {data_size} æ¡")
    
    if data_size > 50000:
        print("âš ï¸  æ•°æ®é‡è¿‡å¤§ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†")
    elif data_size < 100:
        print("âš ï¸  æ•°æ®é‡è¿‡å°ï¼Œå¯èƒ½å½±å“åˆ†æè´¨é‡")
    else:
        print("âœ… æ•°æ®é‡é€‚ä¸­")
    
    return data_size

# ä½¿ç”¨ç¤ºä¾‹
check_data_size("your_project_id")
```

**æ­¥éª¤ 2: æ£€æŸ¥ç³»ç»Ÿèµ„æº**
```bash
# æ£€æŸ¥ CPU ä½¿ç”¨ç‡
top -p $(pgrep -f "business_logic")

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
ps aux | grep business_logic | awk '{print $4, $6}'

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h /tmp  # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶ç©ºé—´
```

**æ­¥éª¤ 3: ä¼˜åŒ–åˆ†æå‚æ•°**
```python
# æ¨èçš„å‚æ•°è®¾ç½®
optimization_params = {
    "small_dataset": {  # < 1000 æ¡
        "min_confidence": 0.6,
        "min_support": 3,
        "max_features": 500,
        "batch_size": 100
    },
    "medium_dataset": {  # 1000-10000 æ¡
        "min_confidence": 0.7,
        "min_support": 5,
        "max_features": 1000,
        "batch_size": 500
    },
    "large_dataset": {  # > 10000 æ¡
        "min_confidence": 0.8,
        "min_support": 10,
        "max_features": 1500,
        "batch_size": 1000
    }
}
```

#### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼**
```python
# ä¿®æ”¹åˆ†æè¯·æ±‚ï¼Œå¯ç”¨æ‰¹å¤„ç†
analysis_request = {
    "project_id": "your_project_id",
    "analysis_types": ["sentiment_correlation"],
    "batch_processing": True,
    "batch_size": 1000,
    "parallel_workers": 4
}
```

**æ–¹æ¡ˆ 2: ä½¿ç”¨ç¼“å­˜**
```python
# å¯ç”¨ç»“æœç¼“å­˜
analysis_request = {
    "project_id": "your_project_id",
    "use_cache": True,
    "cache_ttl": 3600  # 1å°æ—¶ç¼“å­˜
}
```

**æ–¹æ¡ˆ 3: åˆ†æ—¶æ®µåˆ†æ**
```python
# åˆ†æ—¶æ®µåˆ†æå¤§æ•°æ®é›†
import datetime

def analyze_by_time_periods(project_id, days_per_batch=7):
    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=30)
    
    current_date = start_date
    results = []
    
    while current_date < end_date:
        batch_end = current_date + datetime.timedelta(days=days_per_batch)
        
        batch_request = {
            "project_id": project_id,
            "time_range": {
                "start_date": current_date.isoformat(),
                "end_date": batch_end.isoformat()
            }
        }
        
        batch_result = analyze_patterns(batch_request)
        results.append(batch_result)
        
        current_date = batch_end
    
    return merge_analysis_results(results)
```

### 2. åˆ†æç»“æœå¼‚å¸¸

#### é—®é¢˜ç—‡çŠ¶
- åˆ†æç»“æœä¸ºç©ºæˆ–æ•°é‡å¼‚å¸¸å°‘
- ç½®ä¿¡åº¦å¼‚å¸¸ä½æˆ–å¼‚å¸¸é«˜
- è§„åˆ™é€»è¾‘ä¸åˆç†

#### æ’æŸ¥æ­¥éª¤

**æ­¥éª¤ 1: æ•°æ®è´¨é‡æ£€æŸ¥**
```python
def diagnose_data_quality(project_id):
    annotations = get_annotations(project_id)
    df = pd.DataFrame(annotations)
    
    print("=== æ•°æ®è´¨é‡è¯Šæ–­ ===")
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"æ€»è®°å½•æ•°: {len(df)}")
    print(f"å­—æ®µæ•°: {len(df.columns)}")
    
    # ç¼ºå¤±å€¼æ£€æŸ¥
    missing_data = df.isnull().sum()
    print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    for col, missing_count in missing_data.items():
        if missing_count > 0:
            percentage = (missing_count / len(df)) * 100
            print(f"  {col}: {missing_count} ({percentage:.1f}%)")
    
    # æ•°æ®åˆ†å¸ƒæ£€æŸ¥
    if 'sentiment' in df.columns:
        sentiment_dist = df['sentiment'].value_counts()
        print(f"\næƒ…æ„Ÿåˆ†å¸ƒ:")
        for sentiment, count in sentiment_dist.items():
            percentage = (count / len(df)) * 100
            print(f"  {sentiment}: {count} ({percentage:.1f}%)")
        
        # æ£€æŸ¥æ•°æ®å€¾æ–œ
        max_percentage = max(sentiment_dist.values) / len(df) * 100
        if max_percentage > 80:
            print("âš ï¸  æ•°æ®ä¸¥é‡å€¾æ–œï¼Œå¯èƒ½å½±å“åˆ†æè´¨é‡")
    
    # æ–‡æœ¬è´¨é‡æ£€æŸ¥
    if 'text' in df.columns:
        text_lengths = df['text'].str.len()
        print(f"\næ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {text_lengths.mean():.1f}")
        print(f"  æœ€çŸ­: {text_lengths.min()}")
        print(f"  æœ€é•¿: {text_lengths.max()}")
        
        # æ£€æŸ¥ç©ºæ–‡æœ¬
        empty_texts = (text_lengths == 0).sum()
        if empty_texts > 0:
            print(f"âš ï¸  å‘ç° {empty_texts} æ¡ç©ºæ–‡æœ¬")
    
    return df

# ä½¿ç”¨ç¤ºä¾‹
diagnose_data_quality("your_project_id")
```

**æ­¥éª¤ 2: å‚æ•°åˆç†æ€§æ£€æŸ¥**
```python
def validate_analysis_parameters(params):
    issues = []
    
    # æ£€æŸ¥ç½®ä¿¡åº¦é˜ˆå€¼
    if params.get('min_confidence', 0) > 0.9:
        issues.append("ç½®ä¿¡åº¦é˜ˆå€¼è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´ç»“æœè¿‡å°‘")
    elif params.get('min_confidence', 0) < 0.5:
        issues.append("ç½®ä¿¡åº¦é˜ˆå€¼è¿‡ä½ï¼Œå¯èƒ½äº§ç”Ÿå™ªéŸ³ç»“æœ")
    
    # æ£€æŸ¥æ”¯æŒåº¦é˜ˆå€¼
    if params.get('min_support', 0) > 50:
        issues.append("æ”¯æŒåº¦é˜ˆå€¼è¿‡é«˜ï¼Œå¯èƒ½é”™è¿‡é‡è¦æ¨¡å¼")
    elif params.get('min_support', 0) < 2:
        issues.append("æ”¯æŒåº¦é˜ˆå€¼è¿‡ä½ï¼Œå¯èƒ½äº§ç”Ÿå¶ç„¶æ¨¡å¼")
    
    # æ£€æŸ¥æ—¶é—´èŒƒå›´
    if 'time_range' in params:
        start = datetime.datetime.fromisoformat(params['time_range']['start_date'])
        end = datetime.datetime.fromisoformat(params['time_range']['end_date'])
        days_diff = (end - start).days
        
        if days_diff > 365:
            issues.append("æ—¶é—´èŒƒå›´è¿‡é•¿ï¼Œå»ºè®®åˆ†æ®µåˆ†æ")
        elif days_diff < 7:
            issues.append("æ—¶é—´èŒƒå›´è¿‡çŸ­ï¼Œå¯èƒ½æ•°æ®ä¸è¶³")
    
    if issues:
        print("å‚æ•°é—®é¢˜:")
        for issue in issues:
            print(f"  âš ï¸  {issue}")
    else:
        print("âœ… å‚æ•°è®¾ç½®åˆç†")
    
    return len(issues) == 0
```

#### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: æ•°æ®é¢„å¤„ç†**
```python
def preprocess_data_for_analysis(df):
    """æ•°æ®é¢„å¤„ç†ä»¥æé«˜åˆ†æè´¨é‡"""
    
    # 1. æ¸…ç†ç©ºå€¼
    df = df.dropna(subset=['text', 'sentiment'])
    
    # 2. æ–‡æœ¬æ¸…ç†
    df['text'] = df['text'].str.strip()
    df = df[df['text'].str.len() > 0]  # ç§»é™¤ç©ºæ–‡æœ¬
    
    # 3. æ ‡å‡†åŒ–æƒ…æ„Ÿæ ‡ç­¾
    sentiment_mapping = {
        'pos': 'positive',
        'neg': 'negative',
        'neu': 'neutral',
        '1': 'positive',
        '0': 'neutral',
        '-1': 'negative'
    }
    df['sentiment'] = df['sentiment'].map(sentiment_mapping).fillna(df['sentiment'])
    
    # 4. è¿‡æ»¤å¼‚å¸¸é•¿åº¦æ–‡æœ¬
    text_lengths = df['text'].str.len()
    q1, q3 = text_lengths.quantile([0.25, 0.75])
    iqr = q3 - q1
    lower_bound = max(10, q1 - 1.5 * iqr)  # æœ€å°‘10ä¸ªå­—ç¬¦
    upper_bound = q3 + 1.5 * iqr
    
    df = df[(text_lengths >= lower_bound) & (text_lengths <= upper_bound)]
    
    print(f"é¢„å¤„ç†åæ•°æ®é‡: {len(df)}")
    return df
```

**æ–¹æ¡ˆ 2: è°ƒæ•´åˆ†æç­–ç•¥**
```python
def adaptive_analysis_strategy(data_size, data_quality_score):
    """æ ¹æ®æ•°æ®ç‰¹å¾è‡ªé€‚åº”è°ƒæ•´åˆ†æç­–ç•¥"""
    
    if data_size < 100:
        return {
            "min_confidence": 0.5,
            "min_support": 2,
            "analysis_types": ["sentiment_correlation"],  # ç®€åŒ–åˆ†æ
            "use_advanced_nlp": False
        }
    elif data_size < 1000:
        return {
            "min_confidence": 0.6,
            "min_support": 3,
            "analysis_types": ["sentiment_correlation", "keyword_cooccurrence"],
            "use_advanced_nlp": True
        }
    else:
        return {
            "min_confidence": 0.7,
            "min_support": 5,
            "analysis_types": ["sentiment_correlation", "keyword_cooccurrence", 
                             "temporal_trends", "user_behavior"],
            "use_advanced_nlp": True,
            "enable_caching": True
        }
```

### 3. API è°ƒç”¨é—®é¢˜

#### é—®é¢˜ç—‡çŠ¶
- API è¯·æ±‚è¶…æ—¶
- è¿”å› 500 å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
- è®¤è¯å¤±è´¥

#### æ’æŸ¥æ­¥éª¤

**æ­¥éª¤ 1: æ£€æŸ¥ API æœåŠ¡çŠ¶æ€**
```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
curl -X GET "http://localhost:8000/api/business-logic/health"

# æ£€æŸ¥æœåŠ¡æ—¥å¿—
tail -f logs/app.log | grep "business-logic"

# æ£€æŸ¥é”™è¯¯æ—¥å¿—
tail -f logs/errors.log | grep "ERROR"
```

**æ­¥éª¤ 2: éªŒè¯è¯·æ±‚æ ¼å¼**
```python
import requests
import json

def test_api_request():
    url = "http://localhost:8000/api/business-logic/analyze"
    headers = {
        "Authorization": "Bearer your_jwt_token",
        "Content-Type": "application/json"
    }
    
    # æœ€å°åŒ–æµ‹è¯•è¯·æ±‚
    test_data = {
        "project_id": "test_project",
        "analysis_types": ["sentiment_correlation"],
        "min_confidence": 0.7
    }
    
    try:
        response = requests.post(url, json=test_data, headers=headers, timeout=30)
        
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´: {dict(response.headers)}")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… API è°ƒç”¨æˆåŠŸ")
            print(f"è¿”å›æ•°æ®: {json.dumps(result, indent=2, ensure_ascii=False)}")
        else:
            print(f"âŒ API è°ƒç”¨å¤±è´¥: {response.text}")
            
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶")
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥é”™è¯¯")
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")

test_api_request()
```

#### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: é‡å¯æœåŠ¡**
```bash
# åœæ­¢æœåŠ¡
pkill -f "business_logic"

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf /tmp/business_logic_*

# é‡å¯æœåŠ¡
python -m src.business_logic.api &

# éªŒè¯æœåŠ¡å¯åŠ¨
sleep 5
curl -X GET "http://localhost:8000/api/business-logic/health"
```

**æ–¹æ¡ˆ 2: æ£€æŸ¥å’Œä¿®å¤é…ç½®**
```python
# æ£€æŸ¥é…ç½®æ–‡ä»¶
import os
from src.config import settings

def validate_configuration():
    required_settings = [
        'DATABASE_URL',
        'REDIS_URL',
        'JWT_SECRET_KEY'
    ]
    
    missing_settings = []
    for setting in required_settings:
        if not getattr(settings, setting, None):
            missing_settings.append(setting)
    
    if missing_settings:
        print("âŒ ç¼ºå°‘å¿…è¦é…ç½®:")
        for setting in missing_settings:
            print(f"  - {setting}")
        return False
    
    print("âœ… é…ç½®æ£€æŸ¥é€šè¿‡")
    return True

validate_configuration()
```

### 4. å‰ç«¯ç•Œé¢é—®é¢˜

#### é—®é¢˜ç—‡çŠ¶
- é¡µé¢åŠ è½½å¤±è´¥æˆ–ç™½å±
- å›¾è¡¨æ˜¾ç¤ºå¼‚å¸¸
- æ•°æ®æ›´æ–°ä¸åŠæ—¶

#### æ’æŸ¥æ­¥éª¤

**æ­¥éª¤ 1: æ£€æŸ¥æµè§ˆå™¨æ§åˆ¶å°**
```javascript
// åœ¨æµè§ˆå™¨æ§åˆ¶å°ä¸­è¿è¡Œ
console.log("æ£€æŸ¥ JavaScript é”™è¯¯:");
console.log(window.errors || "æ— é”™è¯¯");

// æ£€æŸ¥ç½‘ç»œè¯·æ±‚
console.log("æ£€æŸ¥ç½‘ç»œè¯·æ±‚:");
performance.getEntriesByType("navigation").forEach(entry => {
    console.log(`é¡µé¢åŠ è½½æ—¶é—´: ${entry.loadEventEnd - entry.loadEventStart}ms`);
});

// æ£€æŸ¥ API è°ƒç”¨
fetch('/api/business-logic/health')
    .then(response => response.json())
    .then(data => console.log('API å¥åº·æ£€æŸ¥:', data))
    .catch(error => console.error('API è°ƒç”¨å¤±è´¥:', error));
```

**æ­¥éª¤ 2: æ£€æŸ¥å‰ç«¯æœåŠ¡**
```bash
# æ£€æŸ¥å‰ç«¯æœåŠ¡çŠ¶æ€
curl -X GET "http://localhost:3000"

# æ£€æŸ¥å‰ç«¯æ„å»º
cd frontend
npm run build

# æ£€æŸ¥ä¾èµ–
npm audit
```

#### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: æ¸…ç†ç¼“å­˜å¹¶é‡å¯**
```bash
# æ¸…ç†æµè§ˆå™¨ç¼“å­˜
# Chrome: Ctrl+Shift+Delete
# Firefox: Ctrl+Shift+Delete

# æ¸…ç†å‰ç«¯ç¼“å­˜
cd frontend
rm -rf node_modules/.cache
rm -rf dist/

# é‡æ–°å®‰è£…ä¾èµ–
npm install

# é‡å¯å‰ç«¯æœåŠ¡
npm run dev
```

**æ–¹æ¡ˆ 2: æ£€æŸ¥å’Œä¿®å¤å‰ç«¯é…ç½®**
```javascript
// æ£€æŸ¥å‰ç«¯é…ç½® (frontend/src/config.js)
const config = {
    API_BASE_URL: process.env.REACT_APP_API_BASE_URL || 'http://localhost:8000',
    WS_URL: process.env.REACT_APP_WS_URL || 'ws://localhost:8000/ws',
    TIMEOUT: 30000
};

// éªŒè¯é…ç½®
console.log('å‰ç«¯é…ç½®:', config);

// æµ‹è¯• API è¿æ¥
fetch(`${config.API_BASE_URL}/api/business-logic/health`)
    .then(response => {
        if (response.ok) {
            console.log('âœ… API è¿æ¥æ­£å¸¸');
        } else {
            console.log('âŒ API è¿æ¥å¼‚å¸¸:', response.status);
        }
    })
    .catch(error => {
        console.log('âŒ API è¿æ¥å¤±è´¥:', error);
    });
```

### 5. æ•°æ®åº“ç›¸å…³é—®é¢˜

#### é—®é¢˜ç—‡çŠ¶
- æ•°æ®åº“è¿æ¥å¤±è´¥
- æŸ¥è¯¢è¶…æ—¶
- æ•°æ®ä¸ä¸€è‡´

#### æ’æŸ¥æ­¥éª¤

**æ­¥éª¤ 1: æ£€æŸ¥æ•°æ®åº“è¿æ¥**
```python
import psycopg2
from src.config import settings

def test_database_connection():
    try:
        conn = psycopg2.connect(settings.DATABASE_URL)
        cursor = conn.cursor()
        
        # æµ‹è¯•åŸºæœ¬æŸ¥è¯¢
        cursor.execute("SELECT version();")
        version = cursor.fetchone()
        print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {version[0]}")
        
        # æ£€æŸ¥ä¸šåŠ¡é€»è¾‘ç›¸å…³è¡¨
        tables = ['business_rules', 'business_patterns', 'business_insights']
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table};")
            count = cursor.fetchone()[0]
            print(f"  {table}: {count} æ¡è®°å½•")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

test_database_connection()
```

**æ­¥éª¤ 2: æ£€æŸ¥æ•°æ®åº“æ€§èƒ½**
```sql
-- æ£€æŸ¥æ…¢æŸ¥è¯¢
SELECT query, mean_time, calls, total_time
FROM pg_stat_statements
WHERE query LIKE '%business_%'
ORDER BY mean_time DESC
LIMIT 10;

-- æ£€æŸ¥è¡¨å¤§å°
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables
WHERE tablename LIKE 'business_%'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE tablename LIKE 'business_%'
ORDER BY idx_scan DESC;
```

#### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ 1: ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢**
```python
# æ·»åŠ æŸ¥è¯¢ä¼˜åŒ–
def optimized_rule_query(project_id, limit=100):
    query = """
    SELECT id, name, description, confidence, support, created_at
    FROM business_rules 
    WHERE project_id = %s 
    AND is_active = true
    ORDER BY confidence DESC, support DESC
    LIMIT %s
    """
    
    # ä½¿ç”¨è¿æ¥æ± 
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute(query, (project_id, limit))
            return cursor.fetchall()
```

**æ–¹æ¡ˆ 2: æ•°æ®åº“ç»´æŠ¤**
```sql
-- æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE business_rules;
ANALYZE business_patterns;
ANALYZE business_insights;

-- é‡å»ºç´¢å¼•
REINDEX TABLE business_rules;

-- æ¸…ç†æ— ç”¨æ•°æ®
DELETE FROM business_rules 
WHERE created_at < NOW() - INTERVAL '1 year' 
AND is_active = false;

-- ä¼˜åŒ–è¡¨ç©ºé—´
VACUUM FULL business_rules;
```

## ç›‘æ§å’Œé¢„é˜²

### 1. è®¾ç½®ç›‘æ§å‘Šè­¦

```python
# ç›‘æ§è„šæœ¬ç¤ºä¾‹
import time
import requests
import smtplib
from email.mime.text import MIMEText

def monitor_business_logic_service():
    """ç›‘æ§ä¸šåŠ¡é€»è¾‘æœåŠ¡å¥åº·çŠ¶æ€"""
    
    while True:
        try:
            # å¥åº·æ£€æŸ¥
            response = requests.get(
                "http://localhost:8000/api/business-logic/health",
                timeout=10
            )
            
            if response.status_code != 200:
                send_alert(f"ä¸šåŠ¡é€»è¾‘æœåŠ¡å¼‚å¸¸: HTTP {response.status_code}")
            
            # æ£€æŸ¥å“åº”æ—¶é—´
            if response.elapsed.total_seconds() > 5:
                send_alert(f"ä¸šåŠ¡é€»è¾‘æœåŠ¡å“åº”ç¼“æ…¢: {response.elapsed.total_seconds()}s")
            
            print(f"âœ… æœåŠ¡æ­£å¸¸ - {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
        except Exception as e:
            send_alert(f"ä¸šåŠ¡é€»è¾‘æœåŠ¡ä¸å¯ç”¨: {e}")
        
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

def send_alert(message):
    """å‘é€å‘Šè­¦é‚®ä»¶"""
    # å®ç°é‚®ä»¶å‘é€é€»è¾‘
    print(f"ğŸš¨ å‘Šè­¦: {message}")

# å¯åŠ¨ç›‘æ§
if __name__ == "__main__":
    monitor_business_logic_service()
```

### 2. æ—¥å¿—é…ç½®

```python
# é…ç½®è¯¦ç»†æ—¥å¿—
import logging
import sys

def setup_logging():
    """é…ç½®ä¸šåŠ¡é€»è¾‘æ¨¡å—æ—¥å¿—"""
    
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler('logs/business_logic.log')
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.DEBUG)
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logger = logging.getLogger('business_logic')
    logger.setLevel(logging.DEBUG)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ä½¿ç”¨ç¤ºä¾‹
logger = setup_logging()
logger.info("ä¸šåŠ¡é€»è¾‘æœåŠ¡å¯åŠ¨")
```

### 3. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

def benchmark_analysis_performance():
    """ä¸šåŠ¡é€»è¾‘åˆ†ææ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    test_cases = [
        {"data_size": 100, "expected_time": 5},
        {"data_size": 1000, "expected_time": 30},
        {"data_size": 5000, "expected_time": 120}
    ]
    
    results = []
    
    for case in test_cases:
        print(f"æµ‹è¯•æ•°æ®é‡: {case['data_size']} æ¡")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = generate_test_data(case['data_size'])
        
        # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•
        times = []
        for i in range(3):
            start_time = time.time()
            result = analyze_patterns(test_data)
            end_time = time.time()
            
            execution_time = end_time - start_time
            times.append(execution_time)
            print(f"  ç¬¬ {i+1} æ¬¡: {execution_time:.2f}s")
        
        avg_time = statistics.mean(times)
        std_time = statistics.stdev(times) if len(times) > 1 else 0
        
        results.append({
            "data_size": case['data_size'],
            "avg_time": avg_time,
            "std_time": std_time,
            "expected_time": case['expected_time'],
            "performance_ratio": case['expected_time'] / avg_time
        })
        
        # æ€§èƒ½è¯„ä¼°
        if avg_time <= case['expected_time']:
            print(f"  âœ… æ€§èƒ½è¾¾æ ‡: {avg_time:.2f}s <= {case['expected_time']}s")
        else:
            print(f"  âŒ æ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}s > {case['expected_time']}s")
    
    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
benchmark_results = benchmark_analysis_performance()
```

## è”ç³»æ”¯æŒ

å¦‚æœæŒ‰ç…§æœ¬æŒ‡å—ä»æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒï¼š

### æ”¶é›†è¯Šæ–­ä¿¡æ¯

åœ¨è”ç³»æ”¯æŒå‰ï¼Œè¯·æ”¶é›†ä»¥ä¸‹ä¿¡æ¯ï¼š

```bash
# ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
python -c "
import sys
import platform
import psutil
import pkg_resources

print('=== ç³»ç»Ÿä¿¡æ¯ ===')
print(f'æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}')
print(f'Python ç‰ˆæœ¬: {sys.version}')
print(f'CPU æ ¸æ•°: {psutil.cpu_count()}')
print(f'å†…å­˜æ€»é‡: {psutil.virtual_memory().total / 1024**3:.1f} GB')

print('\n=== ä¾èµ–ç‰ˆæœ¬ ===')
packages = ['fastapi', 'pandas', 'numpy', 'scikit-learn', 'spacy', 'nltk']
for package in packages:
    try:
        version = pkg_resources.get_distribution(package).version
        print(f'{package}: {version}')
    except:
        print(f'{package}: æœªå®‰è£…')

print('\n=== æœåŠ¡çŠ¶æ€ ===')
# æ·»åŠ æœåŠ¡çŠ¶æ€æ£€æŸ¥ä»£ç 
"

# æ”¶é›†æ—¥å¿—
tail -n 100 logs/business_logic.log > diagnostic_logs.txt
tail -n 100 logs/errors.log >> diagnostic_logs.txt
```

### æ”¯æŒæ¸ é“

1. **æŠ€æœ¯æ–‡æ¡£**: æŸ¥çœ‹ [API å‚è€ƒæ–‡æ¡£](api-reference.md)
2. **ç”¨æˆ·æŒ‡å—**: æŸ¥çœ‹ [ä¸šåŠ¡åˆ†æå¸ˆæŒ‡å—](user-guides/business-analyst-guide.md)
3. **GitHub Issues**: æäº¤é—®é¢˜åˆ°é¡¹ç›®ä»“åº“
4. **æŠ€æœ¯æ”¯æŒé‚®ç®±**: support@superinsight.ai

### é—®é¢˜æŠ¥å‘Šæ¨¡æ¿

```
é—®é¢˜æ ‡é¢˜: [ç®€çŸ­æè¿°é—®é¢˜]

ç¯å¢ƒä¿¡æ¯:
- æ“ä½œç³»ç»Ÿ: 
- Python ç‰ˆæœ¬: 
- SuperInsight ç‰ˆæœ¬: 
- æµè§ˆå™¨ (å¦‚é€‚ç”¨): 

é—®é¢˜æè¿°:
[è¯¦ç»†æè¿°é‡åˆ°çš„é—®é¢˜]

é‡ç°æ­¥éª¤:
1. 
2. 
3. 

é¢„æœŸç»“æœ:
[æè¿°æœŸæœ›çš„æ­£å¸¸è¡Œä¸º]

å®é™…ç»“æœ:
[æè¿°å®é™…å‘ç”Ÿçš„æƒ…å†µ]

é”™è¯¯ä¿¡æ¯:
[ç²˜è´´ç›¸å…³çš„é”™è¯¯æ—¥å¿—]

å·²å°è¯•çš„è§£å†³æ–¹æ¡ˆ:
[åˆ—å‡ºå·²ç»å°è¯•è¿‡çš„è§£å†³æ–¹æ³•]

é™„ä»¶:
- è¯Šæ–­æ—¥å¿—æ–‡ä»¶
- æˆªå›¾ (å¦‚é€‚ç”¨)
- é…ç½®æ–‡ä»¶ (å¦‚é€‚ç”¨)
```

---

é€šè¿‡æœ¬æ•…éšœæ’æŸ¥æŒ‡å—ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿå¿«é€Ÿè¯Šæ–­å’Œè§£å†³å¤§éƒ¨åˆ†å¸¸è§é—®é¢˜ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·ä¸è¦çŠ¹è±«è”ç³»æˆ‘ä»¬çš„æŠ€æœ¯æ”¯æŒå›¢é˜Ÿã€‚